# -*- coding: utf-8 -*-
"""AutoEncoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sqb9zTgVueyv3XLwqHrlTp4cFEwT8SW7

# Using Autoencoding for dimensionality reduction in Multi-output regression

## Idea 
Use autoencoder to encode labels into single value, train regressor on this single value, use decoder to decode predicted value into full target set.

###Parameters that seem to work quite good 
lr = 1e-3 , epochs = 800 , 1 Batch Norm, no Dropout arrmse: 0.53
"""

!pip install liac-arff

import torch
import torchvision
from torch import nn
import arff
import numpy as np

DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
num_epochs = 10000
learning_rate = 1e-4 
print_after_epochs = 1000

file = open('rf1.arff')
dataset = arff.load(file)  
d = list(dataset['data'])
data = np.asarray(d,dtype=np.float32)
X = data[:,0:64]
y = data[:,64:72] 
X[np.isnan(X)] = 0 
y[np.isnan(y)] = 0
file.close()
X.astype(np.float64),y.astype(np.float64)  
n_targets = len(y[0])

from sklearn.model_selection import train_test_split
 
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)

y_data = torch.tensor(y_train,dtype=torch.float).to(DEVICE)
y_data_test = torch.tensor(y_test,dtype=torch.float).to(DEVICE)

class autoencoder(nn.Module):
    def __init__(self,n_targets):
        super(autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(n_targets, 512),
            nn.ReLU(True),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(True), 
            nn.Linear(256, 128),
            nn.ReLU(True),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(True),
            nn.Linear(64, 12), 
            nn.ReLU(True), 
            nn.Linear(12, 1))
        self.decoder = nn.Sequential(
            nn.Linear(1, 12),
            nn.ReLU(True),
            nn.Linear(12, 64),
            nn.ReLU(True),
            nn.BatchNorm1d(64),
            nn.Linear(64, 128),
            nn.ReLU(True),
            nn.Linear(128, 256),
            nn.ReLU(True),
            nn.Linear(256, 512),
            nn.Dropout(0.1),
            nn.ReLU(True),
            nn.Linear(512, n_targets))

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

def average__relative_root_mean_squared_error(y_test,y_pred): 
    return sum(np.sqrt(sum((y_test-y_pred)**2) / sum((y_test-np.mean(y_test,axis=0))**2))) / len(y_pred[0,:])

def tensor_average__relative_root_mean_squared_error(y_pred,y_test): 
    return torch.sum(torch.sqrt(torch.sum(torch.pow((y_test-y_pred),2)) / torch.sum(torch.pow((y_test-torch.mean(y_test,dim=0)),2)))) / len(y_pred[0,:])

model = autoencoder(n_targets).to(DEVICE)
print(DEVICE)
criterion = nn.MSELoss() #tensor_average__relative_root_mean_squared_error # nn.MSELoss() 
optimizer = torch.optim.Adam(
    model.parameters(), lr=learning_rate)
model.train()
for epoch in range(num_epochs):
    # ===================forward=====================
    output = model(y_data)
    loss = criterion(output, y_data)
    # ===================backward====================
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    # ===================log========================
    if epoch % print_after_epochs == 0:
      print('epoch [{}/{}], loss:{}'.format(epoch + 1, num_epochs, loss)) 
print('Output:{}'.format(output[1])) 
print('Label:{}'.format(y_data[1]))
print(y_data[1]-output[1])

y_enc_train = model.encoder(y_data)
y_pred_test_enc = model.encoder(y_data_test) 
test_loss_enc = criterion(y_pred_test_enc,y_data_test) 
print(test_loss_enc) 

from sklearn.ensemble import GradientBoostingRegressor 

reg = GradientBoostingRegressor().fit(X_train,y_enc_train.cpu().detach().numpy()) 
y_pred_test = reg.predict(X_test)
y_pred_test_t = torch.tensor(y_pred_test,dtype=torch.float).unsqueeze(1).to(DEVICE)

y_pred_dec = model.decoder(y_pred_test_t) 
loss = criterion(y_pred_dec,y_data_test) 
print(loss)  
print(average__relative_root_mean_squared_error(y_pred_dec.cpu().detach().numpy(),y_data_test.cpu().detach().numpy()))